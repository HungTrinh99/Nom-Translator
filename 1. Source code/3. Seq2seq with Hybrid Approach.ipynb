{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2seq with Hybrid approach.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6QMkUsTMMHm",
        "colab_type": "text"
      },
      "source": [
        "# installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv2WUDqvJi0f",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/bentrevett/pytorch-seq2seq\n",
        "https://github.com/oxford-cs-deepnlp-2017/lectures\n",
        "http://www.phontron.com/class/nn4nlp2019/assignments.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD2w6S-4ra-i",
        "colab_type": "code",
        "outputId": "4b55e175-05d0-4b93-bfdb-cded6c6de6e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!pip install torch==1.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K     |████████████████████████████████| 591.8MB 31kB/s \n",
            "\u001b[31mERROR: torchvision 0.4.2 has requirement torch==1.3.1, but you'll have torch 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Found existing installation: torch 1.3.1\n",
            "    Uninstalling torch-1.3.1:\n",
            "      Successfully uninstalled torch-1.3.1\n",
            "Successfully installed torch-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM-_v96isz_1",
        "colab_type": "code",
        "outputId": "ff83d292-e1e5-474f-8bd1-8bc9657f6a3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 8265018910091821816, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 1824534260472105775\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 3221333456112038677\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15956161332\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 16314864333158252133\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXP5S2zTsgHn",
        "colab_type": "code",
        "outputId": "58b6411b-a90e-4ad6-cc10-d9f252bf376d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quk2RIkSR0Ww",
        "colab_type": "text"
      },
      "source": [
        "# utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJPgCYOXMK21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5rYpidBMXiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sents_char(sents: List[List[List[int]]], char_pad_token: int) -> List[List[List[int]]]:\n",
        "    \"\"\" Pad list of sentences according to the longest sentence in the batch and max_word_length.\n",
        "    @param sents (list[list[list[int]]]): list of sentences, result of `words2charindices()`\n",
        "        from `vocab.py`\n",
        "    @param char_pad_token (int): index of the character-padding token\n",
        "    @returns sents_padded (list[list[list[int]]]): list of sentences where sentences/words shorter\n",
        "        than the max length sentence/word are padded out with the appropriate pad token, such that\n",
        "        each sentence in the batch now has same number of words and each word has an equal\n",
        "        number of characters\n",
        "        Output shape: (batch_size, max_sentence_length, max_word_length)\n",
        "\n",
        "    In this function: bacth_size is the number of sentences returned from batch_iter\n",
        "                      max_sentence_length is the length of the longest sentence among senteneces within\n",
        "                      the batch\n",
        "                      max_word_length is pre-defined\n",
        "    \"\"\"\n",
        "    # Words longer than 21 characters should be truncated\n",
        "    max_word_length = 21\n",
        "\n",
        "    padded_sents = list()\n",
        "\n",
        "    max_sent_len = max(len(sent) for sent in sents)\n",
        "    word_pad_tokens = [char_pad_token] * max_word_length\n",
        "\n",
        "    for sent in sents:\n",
        "        # Pad word\n",
        "        padded_words = list()\n",
        "        for word in sent:\n",
        "            if len(word) <= max_word_length:\n",
        "                padded_word = word + [char_pad_token] * (max_word_length - len(word))\n",
        "                padded_words.append(padded_word)\n",
        "            else:\n",
        "                truncated_word = word[:max_word_length]\n",
        "                word.append(truncated_word)\n",
        "        # Pad sentence\n",
        "        padded_sent = padded_words + [word_pad_tokens] * (max_sent_len - len(padded_words))\n",
        "        padded_sents.append(padded_sent)\n",
        "    \n",
        "    #print(80*'-')\n",
        "    #print('padded_sents: ', padded_sents)\n",
        "    #print('-'*80)\n",
        "\n",
        "    return padded_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu_R_L0DMfLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sents(sents: List[List[int]], pad_token: int):\n",
        "    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n",
        "    @param sents (list[list[int]]): list of sentences, where each sentence\n",
        "                                    is represented as a list of words\n",
        "    @param pad_token (int): padding token\n",
        "    @returns sents_padded (list[list[int]]): list of sentences where sentences shorter\n",
        "        than the max length sentence are padded out with the pad_token, such that\n",
        "        each sentences in the batch now has equal length.\n",
        "        Output shape: (batch_size, max_sentence_length)\n",
        "    \"\"\"\n",
        "    sents_padded = []\n",
        "\n",
        "    padded_sentences = []\n",
        "\n",
        "    max_len_sents = max([len(sent) for sent in sents])\n",
        "    for sent in sents:\n",
        "    \t# Pad sentence\n",
        "        padded_sent = sent + (max_len_sents - len(sent)) * [pad_token]\n",
        "        padded_sentences.append(padded_sent)\n",
        "\n",
        "    #print(80*'-')\n",
        "    #print('padded_sentences: ', padded_sentences)\n",
        "    #print('-'*80)\n",
        "    return padded_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDRW4G0iMj2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_corpus(file_path, source):\n",
        "    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n",
        "    @param file_path (str): path to file containing corpus\n",
        "    @param source (str): \"tgt\" or \"src\" indicating whether text\n",
        "        is of the source language or target language\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for line in open(file_path, encoding = 'utf-8-sig'):\n",
        "        sent = line.strip().split(' ')\n",
        "        # only append <s> and </s> to the target sentence\n",
        "        if source == 'tgt':\n",
        "            sent = ['<s>'] + sent + ['</s>']\n",
        "        data.append(sent)\n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTHuFXxvMizW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_iter(data, batch_size, shuffle=False):   \n",
        "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
        "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
        "    @param batch_size (int): batch size\n",
        "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
        "    \"\"\"\n",
        "    batch_num = math.ceil(len(data) / batch_size)\n",
        "    index_array = list(range(len(data)))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(index_array)\n",
        "\n",
        "    for i in range(batch_num):\n",
        "        indices = index_array[i * batch_size: (i + 1) * batch_size]\n",
        "        examples = [data[idx] for idx in indices]\n",
        "\n",
        "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
        "        src_sents = [e[0] for e in examples]\n",
        "        tgt_sents = [e[1] for e in examples]\n",
        "\n",
        "        yield src_sents, tgt_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPQTL7QkMFzT",
        "colab_type": "text"
      },
      "source": [
        "# vocab.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn7d1Kx0hoKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from docopt import docopt\n",
        "from itertools import chain\n",
        "import json\n",
        "import torch\n",
        "from typing import List"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YVen7kAMvTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VocabEntry(object):\n",
        "    def __init__(self, word2id = None):\n",
        "        \"\"\" Init VocabEntry Instance.\n",
        "        @param word2id (dict): dictionary mapping words 2 indices\n",
        "        \"\"\"\n",
        "        if word2id:\n",
        "            self.word2id = word2id\n",
        "        else:\n",
        "            self.word2id = dict()\n",
        "            self.word2id['<pad>'] = 0   # Pad Token\n",
        "            self.word2id['<s>'] = 1 # Start Token\n",
        "            self.word2id['</s>'] = 2    # End Token\n",
        "            self.word2id['<unk>'] = 3   # Unknown Token\n",
        "        self.unk_id = self.word2id['<unk>']\n",
        "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
        "        \n",
        "        ## Additions to the A4 code:\n",
        "        self.char_list = list(\"\"\"0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]“”＂\"\"\") \n",
        "        self.char_list.extend(['a','ă','â','b','c','d','đ','e','ê','g','h','i','k','l',\n",
        "                               'm','n','o','ô','ơ','p','q','r','s','t','u','ư','v','x','y'])\n",
        "        self.char_list.extend(['á','à','ả','ã','ạ','é','è','ẻ','ẽ','ẹ','ế','ề','ể','ễ','ệ',\n",
        "                               'ó','ò','ỏ','õ','ọ','ố','ồ','ổ','ỗ','ộ','ớ','ờ','ở','ỡ','ợ',\n",
        "                               'ú','ù','ủ','ũ','ụ','ứ','ừ','ử','ữ','ự','ý','ỳ','ỷ','ỹ','ỵ',\n",
        "                               'í','ì','ỉ','ĩ','ị','ấ','ầ','ẩ','ẫ','ậ','ắ','ằ','ẳ','ẵ','ặ'])\n",
        "        self.char_list.extend(['A','B','C','D','Đ','E','Ê','G','H','I','K','L','M','N',\n",
        "                               'O','Ô','Ơ','P','Q','R','S','T','U','Ư','V','X','Y','Ă','Â'])\n",
        "        self.char_list.extend(['Á','À','Ả','Ã','Ạ','É','È','Ẻ','Ẽ','Ẹ','Ế','Ề','Ể','Ễ','Ệ',\n",
        "                                'Ó','Ò','Ỏ','Õ','Ọ','Ố','Ồ','Ổ','Ỗ','Ộ','Ớ','Ờ','Ở','Ỡ','Ợ',\n",
        "                                'Ú','Ù','Ủ','Ũ','Ụ','Ứ','Ừ','Ử','Ữ','Ự','Ý','Ỳ','Ỷ','Ỹ','Ỵ',\n",
        "                                'Í','Ì','Ỉ','Ĩ','Ị','Ấ','Ầ','Ẩ','Ẫ','Ậ','Ắ','Ằ','Ẳ','Ẵ','Ặ'])\n",
        "\n",
        "        self.char2id = dict() # Converts characters to integers\n",
        "        self.char2id['<pad>'] = 0\n",
        "        self.char2id['{'] = 1\n",
        "        self.char2id['}'] = 2\n",
        "        self.char2id['<unk>'] = 3\n",
        "\n",
        "        for i, c in enumerate(self.char_list):\n",
        "            self.char2id[c] = len(self.char2id)\n",
        "\n",
        "        self.char_unk = self.char2id['<unk>']\n",
        "        self.start_of_word = self.char2id[\"{\"]\n",
        "        self.end_of_word = self.char2id[\"}\"]\n",
        "\n",
        "        assert self.start_of_word + 1 == self.end_of_word\n",
        "\n",
        "        self.id2char = {v: k for k, v in self.char2id.items()} # Converts integers to characters\n",
        "        ## End additions to the A4 code\n",
        "\n",
        "    def __getitem__(self, word):\n",
        "        \"\"\" Retrieve word's index. Return the index for the unk\n",
        "        token if the word is out of vocabulary.\n",
        "        @param word (str): word to look up.\n",
        "        @returns index (int): index of word \n",
        "        \"\"\"\n",
        "        return self.word2id.get(word, self.unk_id)\n",
        "\n",
        "    def __contains__(self, word):\n",
        "        \"\"\" Check if word is captured by VocabEntry.\n",
        "        @param word (str): word to look up\n",
        "        @returns contains (bool): whether word is contained    \n",
        "        \"\"\"\n",
        "        return word in self.word2id\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        \"\"\" Raise error, if one tries to edit the VocabEntry.\n",
        "        \"\"\"\n",
        "        raise ValueError('vocabulary is readonly')\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" Compute number of words in VocabEntry.\n",
        "        @returns len (int): number of words in VocabEntry\n",
        "        \"\"\"\n",
        "        return len(self.word2id)\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\" Representation of VocabEntry to be used\n",
        "        when printing the object.\n",
        "        \"\"\"\n",
        "        return 'Vocabulary[size=%d]' % len(self)\n",
        "\n",
        "    def id2word(self, wid):\n",
        "        \"\"\" Return mapping of index to word.\n",
        "        @param wid (int): word index\n",
        "        @returns word (str): word corresponding to index\n",
        "        \"\"\"\n",
        "        return self.id2word[wid]\n",
        "\n",
        "    def add(self, word):\n",
        "        \"\"\" Add word to VocabEntry, if it is previously unseen.\n",
        "        @param word (str): word to add to VocabEntry\n",
        "        @return index (int): index that the word has been assigned\n",
        "        \"\"\"\n",
        "        if word not in self:\n",
        "            wid = self.word2id[word] = len(self)\n",
        "            self.id2word[wid] = word\n",
        "            return wid\n",
        "        else:\n",
        "            return self[word]\n",
        "\n",
        "    def words2charindices(self, sents: List[List[str]]) -> List[List[List[int]]]:\n",
        "        \"\"\" Convert list of sentences of words into list of list of list of character indices.\n",
        "\n",
        "            The first two steps : SPLITTING & VOCABULARY LOOKUP\n",
        "\n",
        "        @param sents (list[list[str]]): sentence(s) in words\n",
        "        @return char_ids (list[list[list[int]]]): sentence(s) in indices\n",
        "        \"\"\"\n",
        "        # List of list of list of words with '{' and '}' between each word\n",
        "        sents_list_words = [ [ ['{' + word + '}'] for word in sent ] for sent in sents ]\n",
        "        # List of list of list of indices \n",
        "        # Each word is now: ['word'], a list of one word\n",
        "        char_ids = [ [ [self.char2id[char] for char in word[0]] for word in sent] for sent in sents_list_words]\n",
        "\n",
        "        return char_ids\n",
        "\n",
        "    def words2indices(self, sents: List[List[str]]) -> List[List[int]]:\n",
        "        \"\"\" Convert list of sentences of words into list of list of indices.\n",
        "        @param sents (list[list[str]]): sentence(s) in words\n",
        "        @return word_ids (list[list[int]]): sentence(s) in indices\n",
        "        \"\"\"\n",
        "        return [[self[w] for w in s] for s in sents]\n",
        "\n",
        "    def indices2words(self, word_ids: List[int]) -> List[str]:\n",
        "        \"\"\" Convert list of indices into words.\n",
        "        @param word_ids (list[int]): list of word ids\n",
        "        @return sents (list[str]): list of words\n",
        "        \"\"\"\n",
        "        return [self.id2word[w_id] for w_id in word_ids]\n",
        "\n",
        "    def to_input_tensor_char(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
        "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n",
        "        shorter sentences.\n",
        "        @param sents (List[List[str]]): list of sentences (words)\n",
        "        @param device: device on which to load the tensor, i.e. CPU or GPU\n",
        "        @returns sents_var: tensor of (max_sentence_length, batch_size, max_word_length)\n",
        "        \n",
        "        max_sent_len is the number of words including <pad>'s\n",
        "        max_word_length is the number of characters includng <pad>'s\n",
        "        batch_size is the number of sentences\n",
        "        \n",
        "        Tensor of shape (batch_size, max_sent_len, max_word_len) is more easily interpretable\n",
        "        for human, but this function returned value is fed into pack_padded_sequence eventually\n",
        "        and this function requires its input's shape to be (longest_len, batch, )\n",
        "        \"\"\"\n",
        "        #print(80*'-')\n",
        "        #print('sents: ', sents)\n",
        "        #print(80*'-')\n",
        "        char_ids = self.words2charindices(sents)\n",
        "        #print(80*'-')\n",
        "        #print('char_ids: ', char_ids)\n",
        "        #print(80*'-')\n",
        "        sents_t = pad_sents_char(char_ids, self.char2id['<pad>']) # (batch_size, max_sent_len, max_word_len)\n",
        "        #print(80*'-')\n",
        "        #print('sents_t: ', sents_t)\n",
        "        #print(80*'-')\n",
        "        chars_var = torch.tensor(sents_t, dtype = torch.long, device = device)\n",
        "        #print(80*'-')\n",
        "        #print('chars_var:', chars_var)\n",
        "        #print('-'*80)\n",
        "        # Cannot use torch.t(chars_var) 'cause torch.t() only transposes tensor <= 2-D\n",
        "\n",
        "        chars_var = chars_var.permute(1, 0, 2) # (max_sent_len, batch_size, max_word_len)\n",
        "\n",
        "        return chars_var\n",
        "\n",
        "    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
        "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n",
        "        shorter sentences.\n",
        "\n",
        "        @param sents (List[List[str]]): list of sentences (words)\n",
        "        @param device: device on which to load the tesnor, i.e. CPU or GPU\n",
        "\n",
        "        @returns sents_var: tensor of (max_sentence_length, batch_size)\n",
        "        \"\"\"\n",
        "        word_ids = self.words2indices(sents)\n",
        "        sents_t = pad_sents(word_ids, self['<pad>'])\n",
        "\n",
        "        sents_var = torch.tensor(sents_t, dtype = torch.long, device = device) # (batch_size, max_sent_length)\n",
        "\n",
        "        return torch.t(sents_var) # Transpose\n",
        "\n",
        "    @staticmethod\n",
        "    def from_corpus(corpus, size, freq_cutoff = 2):\n",
        "        \"\"\" Given a corpus construct a Vocab Entry.\n",
        "        @param corpus (list[str]): corpus of text produced by read_corpus function\n",
        "        @param size (int): # of words in vocabulary\n",
        "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n",
        "        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n",
        "        \"\"\"\n",
        "        vocab_entry = VocabEntry()\n",
        "        # Treat consecutive sentences as a single sentence and create a mapping between\n",
        "        # word and its corresponding frequency\n",
        "        word_freq = Counter(chain(*corpus))\n",
        "        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n",
        "\n",
        "        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n",
        "              .format(len(word_freq), freq_cutoff, len(valid_words)))\n",
        "\n",
        "        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n",
        "\n",
        "        for word in top_k_words:\n",
        "            vocab_entry.add(word)\n",
        "\n",
        "        return vocab_entry\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SS4-Q6_NEt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab(object):\n",
        "    \"\"\" Vocab encapsulating src and target langauges.\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n",
        "        \"\"\" Init Vocab.\n",
        "        @param src_vocab (VocabEntry): VocabEntry for source language\n",
        "        @param tgt_vocab (VocabEntry): VocabEntry for target language\n",
        "        \"\"\"\n",
        "        self.src = src_vocab\n",
        "        self.tgt = tgt_vocab\n",
        "\n",
        "    def get_word2id(self):\n",
        "        return self.src.word2id, self.tgt.word2id\n",
        "\n",
        "    @staticmethod\n",
        "    def build(src_sents, tgt_sents, vocab_size, freq_cutoff) -> 'Vocab':\n",
        "        \"\"\" Build Vocabulary.\n",
        "        @param src_sents (list[str]): Source sentences provided by read_corpus() function\n",
        "        @param tgt_sents (list[str]): Target sentences provided by read_corpus() function\n",
        "        @param vocab_size (int): Size of vocabulary for both source and target languages\n",
        "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word.\n",
        "        \"\"\"\n",
        "        assert len(src_sents) == len(tgt_sents)\n",
        "\n",
        "        print('initialize source vocabulary ..')\n",
        "        src = VocabEntry.from_corpus(src_sents, vocab_size, freq_cutoff)\n",
        "\n",
        "        print('initialize target vocabulary ..')\n",
        "        tgt = VocabEntry.from_corpus(tgt_sents, vocab_size, freq_cutoff)\n",
        "\n",
        "        return Vocab(src, tgt)\n",
        "\n",
        "    def save(self, file_path):\n",
        "        \"\"\" Save Vocab to file as JSON dump.\n",
        "        @param file_path (str): file path to vocab file\n",
        "        \"\"\"\n",
        "        json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), open(file_path, 'w'), indent=2)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(file_path):\n",
        "        \"\"\" Load vocabulary from JSON dump.\n",
        "        @param file_path (str): file path to vocab file\n",
        "        @returns Vocab object loaded from JSON dump\n",
        "        \"\"\"\n",
        "        entry = json.load(open(file_path, 'r'))\n",
        "        src_word2id = entry['src_word2id']\n",
        "        tgt_word2id = entry['tgt_word2id']\n",
        "\n",
        "        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\" Representation of Vocab to be used\n",
        "        when printing the object.\n",
        "        \"\"\"\n",
        "        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbT9AUvmoX5X",
        "colab_type": "text"
      },
      "source": [
        "# source_model_embeddings.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPMF0eCnNXEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNlAZrHNOUUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsjqJPQuoatR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SourceModelEmbeddings(nn.Module): \n",
        "    \"\"\"\n",
        "    Class that converts input words to their embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size, vocab):\n",
        "        \"\"\"\n",
        "        Init the Embedding layers.\n",
        "\n",
        "        @param embed_size (int): Embedding size (dimensionality)\n",
        "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
        "                              See vocab.py for documentation.\n",
        "        \"\"\"\n",
        "        super(SourceModelEmbeddings, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        # default values\n",
        "        self.source = None\n",
        "\n",
        "        src_pad_token_idx = vocab.src['<pad>']\n",
        "        \n",
        "        self.source = nn.Embedding(num_embeddings = len(vocab.src), embedding_dim = self.embed_size,\n",
        "                                    padding_idx =  src_pad_token_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxpuVBI4Ov8r",
        "colab_type": "text"
      },
      "source": [
        "# target_model_embeddings.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEVznGGPOpMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48kiZbKJO35l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TargetModelEmbeddings(nn.Module): \n",
        "    \"\"\"\n",
        "    Class that converts input words to their embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size, vocab):\n",
        "        \"\"\"\n",
        "        Init the Embedding layers.\n",
        "\n",
        "        @param embed_size (int): Embedding size (dimensionality)\n",
        "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
        "                              See vocab.py for documentation.\n",
        "        \"\"\"\n",
        "        super(TargetModelEmbeddings, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        # default values\n",
        "        self.target = None\n",
        "\n",
        "        tgt_pad_token_idx = vocab.tgt['<pad>']\n",
        "        \n",
        "        self.target = nn.Embedding(num_embeddings = len(vocab.src), embedding_dim = self.embed_size,\n",
        "                                    padding_idx =  tgt_pad_token_idx)\n",
        "            \n",
        "    def forward(self, data):\n",
        "      return self.target(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgVT8qB3hvKe",
        "colab_type": "text"
      },
      "source": [
        "# Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwxT2NTuhxWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "root = '/content/drive/My Drive/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqO8bkOXhLxk",
        "colab_type": "text"
      },
      "source": [
        "# nmt_model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OTKrcuHSgFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple\n",
        "import sys\n",
        "from typing import List, Tuple, Dict, Set, Union\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "\n",
        "Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmgiRK27PEBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NMT(nn.Module):\n",
        "    \"\"\" Simple Neural Machine Translation Model:\n",
        "        - Bidrectional LSTM Encoder\n",
        "        - Unidirection LSTM Decoder\n",
        "        - Global Attention Model (Luong, et al. 2015)\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2, no_char_decoder=False):\n",
        "        \"\"\" Init NMT Model.\n",
        "\n",
        "        @param embed_size (int): Embedding size (dimensionality)\n",
        "        @param hidden_size (int): Hidden Size (dimensionality)\n",
        "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
        "                              See vocab.py for documentation.\n",
        "        @param dropout_rate (float): Dropout probability, for attention\n",
        "        \"\"\"\n",
        "        super(NMT, self).__init__()\n",
        "\n",
        "        self.model_embeddings_source = SourceModelEmbeddings(embed_size, vocab)\n",
        "        self.model_embeddings_target = TargetModelEmbeddings(embed_size, vocab)\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.vocab = vocab\n",
        "\n",
        "        # default values\n",
        "        self.encoder = None \n",
        "        self.decoder = None\n",
        "        self.h_projection = None\n",
        "        self.c_projection = None\n",
        "        self.att_projection = None\n",
        "        self.combined_output_projection = None\n",
        "        self.target_vocab_projection = None\n",
        "        self.dropout = None\n",
        "\n",
        "        # Input of this layer is ((seq_len, batch, input_size), h, c)\n",
        "        features_in_input = embed_size # Size of each word embedding\n",
        "        features_in_hidden = self.hidden_size\n",
        "        self.encoder = nn.LSTM(input_size = features_in_input, hidden_size = features_in_hidden, \n",
        "                                bias = True, bidirectional = True, num_layers = 1)\n",
        "\n",
        "        features_in_input = embed_size + self.hidden_size # y_bar_t\n",
        "        features_in_hidden = self.hidden_size \n",
        "        self.decoder = nn.LSTMCell(input_size = features_in_input, hidden_size = features_in_hidden, \n",
        "                                   bias = True)\n",
        "\n",
        "        # In the note, we have W * x + b, but here, the formular is x * W + b\n",
        "\n",
        "        # Perform: input * W_h_projection\n",
        "        self.h_projection = nn.Linear(in_features = 2 * self.hidden_size, \n",
        "                                      out_features = self.hidden_size, bias = False)\n",
        "\n",
        "        # Perform: input * W_c_projection\n",
        "        self.c_projection = nn.Linear(in_features = 2 * self.hidden_size, \n",
        "                                      out_features = self.hidden_size, bias = False)\n",
        "\n",
        "        self.att_projection = nn.Linear(in_features = 2 * self.hidden_size,\n",
        "                                        out_features = self.hidden_size, bias = False)\n",
        "\n",
        "        self.combined_output_projection = nn.Linear(in_features = 3 * self.hidden_size, \n",
        "                                                    out_features = self.hidden_size, bias = False)\n",
        "\n",
        "        self.target_vocab_projection = nn.Linear(in_features = self.hidden_size, \n",
        "                                                 out_features = len(vocab.tgt), bias = False)\n",
        "\n",
        "        self.dropout = nn.Dropout(p = self.dropout_rate)\n",
        "\n",
        "        if not no_char_decoder:\n",
        "           self.charDecoder = CharDecoder(hidden_size, target_vocab=vocab.tgt)\n",
        "        else:\n",
        "           self.charDecoder = None\n",
        "\n",
        "    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n",
        "        \"\"\" Take a mini-batch of source and target sentences, compute the log-likelihood of\n",
        "        target sentences under the language models learned by the NMT system.\n",
        "\n",
        "        @param source (List[List[str]]): list of source sentence tokens\n",
        "        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`\n",
        "\n",
        "        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n",
        "                                    log-likelihood of generating the gold-standard target sentence for\n",
        "                                    each example in the input batch. Here b = batch size.\n",
        "        \"\"\"\n",
        "        # Compute sentence lengths before applying padding\n",
        "        source_lengths = [len(s) for s in source]\n",
        "\n",
        "        # Convert list of lists into tensors\n",
        "\n",
        "        source_padded_chars = self.vocab.src.to_input_tensor(source, device = self.device) # (src_len, batch, max_word_len)\n",
        "        target_padded_chars = self.vocab.tgt.to_input_tensor_char(target, device = self.device) # (tgt_len, batch, max_word_len)\n",
        "        # For predictions\n",
        "        target_padded = self.vocab.tgt.to_input_tensor(target, device = self.device) # (tgt_len, b)\n",
        "\n",
        "        enc_hiddens, dec_init_state = self.encode(source_padded_chars, source_lengths)\n",
        "        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n",
        "        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n",
        "\n",
        "        # (sent_max_len, batch_size, vocab_size)\n",
        "        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)\n",
        "\n",
        "        # Zero out, probabilities for which we have nothing in the target text\n",
        "        target_masks = (target_padded != self.vocab.tgt['<pad>']).float()\n",
        "\n",
        "        # Compute log probability of generating true target words\n",
        "        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), \n",
        "                                                  dim=-1).squeeze(-1) * target_masks[1:]\n",
        "        scores = target_gold_words_log_prob.sum() # mhahn2 Small modification from A4 code.\n",
        "\n",
        "        if self.charDecoder is not None:\n",
        "            max_word_len = target_padded_chars.shape[2]\n",
        "            # Exclude <pad> token\n",
        "            target_words = target_padded[1:].contiguous().view(-1)\n",
        "            target_chars = target_padded_chars[1:].contiguous().view(-1, max_word_len)\n",
        "            target_outputs = combined_outputs.view(-1, self.hidden_size)\n",
        "\n",
        "            target_chars_oov = target_chars \n",
        "            rnn_states_oov = target_outputs \n",
        "            oovs_losses = self.charDecoder.train_forward(target_chars_oov.t(), \n",
        "                    (rnn_states_oov.unsqueeze(0), rnn_states_oov.unsqueeze(0)))\n",
        "            scores = scores - oovs_losses\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n",
        "            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n",
        "        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b, max_word_length), where\n",
        "                                        b = batch_size, src_len = maximum source sentence length. Note that\n",
        "                                       these have already been sorted in order of longest to shortest sentence.\n",
        "        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n",
        "        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n",
        "                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
        "        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n",
        "                                                hidden state and cell.\n",
        "        \"\"\"\n",
        "        enc_hiddens, dec_init_state = None, None\n",
        "\n",
        "        # Look up word embeddings from an embedding matrix\n",
        "        X = self.model_embeddings_source.source(source_padded)\n",
        "        # Because RNN module takes PackedSequence as input\n",
        "        # https://www.kdnuggets.com/2018/06/taming-lstms-variable-sized-mini-batches-pytorch.html\n",
        "        packed_X = pack_padded_sequence(X, lengths = source_lengths)\n",
        "        enc_hiddens, (last_hidden, last_cell) = self.encoder(packed_X)\n",
        "\n",
        "        # Inverse operation of pack_padded_sequence\n",
        "        # Returned value: (padded sentence: tuple of Tensor, list of lengths of each sentence in the batch: Tensor)\n",
        "        padded_enc_hiddens, _ = pad_packed_sequence(sequence = enc_hiddens, batch_first = True)\n",
        "        # If input of encoder (nn.LSTM) is PackedSequence object, the output must be unpacked\n",
        "        enc_hiddens = padded_enc_hiddens\n",
        "\n",
        "        forwards, backwards = last_hidden[0][:], last_hidden[1][:]\n",
        "        concatenated_last_hidden = torch.cat(tensors = (forwards, backwards), dim = 1)\n",
        "        init_decoder_hidden = self.h_projection(concatenated_last_hidden)\n",
        "\n",
        "        forwards, backwards = last_cell[0][:], last_cell[1][:]\n",
        "        concatenated_last_cell = torch.cat(tensors = (forwards, backwards), dim = 1)\n",
        "        init_decoder_cell = self.c_projection(concatenated_last_cell)\n",
        "\n",
        "        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n",
        "\n",
        "        return enc_hiddens, dec_init_state\n",
        "\n",
        "    def decode(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor, dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute combined output vectors for a batch.\n",
        "        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n",
        "                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
        "        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where\n",
        "                                     b = batch size, src_len = maximum source sentence length.\n",
        "        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n",
        "        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b, max_word_length), where\n",
        "                                       tgt_len = maximum target sentence length, b = batch size.\n",
        "        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where\n",
        "                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n",
        "        \"\"\"\n",
        "        # Chop of the <END> token for max length sentences.\n",
        "        target_padded = target_padded[:-1] # reduce the first dimension by 1\n",
        "\n",
        "        # Initialize the decoder state (hidden and cell)\n",
        "        dec_state = dec_init_state\n",
        "\n",
        "        # Initialize previous combined output vector o_{t-1} as zero\n",
        "        batch_size = enc_hiddens.size(0)\n",
        "        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n",
        "\n",
        "        # Initialize a list we will use to collect the combined output o_t on each step\n",
        "        combined_outputs = []\n",
        "\n",
        "        enc_hiddens_proj = self.att_projection(enc_hiddens)\n",
        "\n",
        "        # Look up target word embeddings\n",
        "        Y = self.model_embeddings_target(target_padded)\n",
        "\n",
        "        for Y_t in torch.split(Y, split_size_or_sections = 1):\n",
        "            # Dimension of size 1 removed\n",
        "            Y_t = torch.squeeze(Y_t, dim = 0)\n",
        "            Ybar_t = torch.cat((Y_t, o_prev), dim = 1)\n",
        "            dec_state, o_t, _ = self.step(Ybar_t, dec_state, enc_hiddens,\n",
        "                                          enc_hiddens_proj, enc_masks)\n",
        "            combined_outputs.append(o_t)\n",
        "            o_prev = o_t\n",
        "\n",
        "        combined_outputs = torch.stack(combined_outputs)\n",
        "\n",
        "        return combined_outputs\n",
        "\n",
        "    def step(self, Ybar_t: torch.Tensor, dec_state: Tuple[torch.Tensor, torch.Tensor], enc_hiddens: torch.Tensor, enc_hiddens_proj: torch.Tensor, enc_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\" Compute one forward step of the LSTM decoder, including the attention computation.\n",
        "        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n",
        "                                where b = batch size, e = embedding size, h = hidden size.\n",
        "        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n",
        "                First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.\n",
        "        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n",
        "                                    src_len = maximum source length, h = hidden size.\n",
        "        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n",
        "                                    where b = batch size, src_len = maximum source length, h = hidden size.\n",
        "        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),\n",
        "                                    where b = batch size, src_len is maximum source length.\n",
        "        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n",
        "                First tensor is decoder's new hidden state, second tensor is decoder's new cell.\n",
        "        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n",
        "        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n",
        "                                Note: You will not use this outside of this function.\n",
        "                                      We are simply returning this value so that we can sanity check\n",
        "                                      your implementation.\n",
        "        \"\"\"\n",
        "\n",
        "        combined_output = None\n",
        "        # new dec_state\n",
        "        dec_state = self.decoder(Ybar_t, dec_state)\n",
        "        (dec_hidden, dec_cell) = dec_state\n",
        "\n",
        "        dec_hidden = torch.unsqueeze(dec_hidden, dim = 2)\n",
        "        # Attention scores\n",
        "        e_t = torch.bmm(enc_hiddens_proj, dec_hidden)\n",
        "        e_t = torch.squeeze(e_t, dim = 2)\n",
        "\n",
        "        # Set e_t to -inf where enc_masks has 1\n",
        "        if enc_masks is not None:\n",
        "            e_t.data.masked_fill_(enc_masks.byte(), -float('inf'))\n",
        "\n",
        "        # Attention distribution\n",
        "        alpha_t = F.softmax(e_t, dim = 1)\n",
        "        \n",
        "        alpha_t = torch.unsqueeze(alpha_t, dim = 1)\n",
        "        a_t = torch.bmm(alpha_t, enc_hiddens)\n",
        "        a_t = torch.squeeze(a_t, dim = 1)\n",
        "\n",
        "        dec_hidden = torch.squeeze(dec_hidden, dim = 2)\n",
        "        U_t = torch.cat((dec_hidden, a_t), dim = 1)\n",
        "\n",
        "        V_t = self.combined_output_projection(U_t)\n",
        "\n",
        "        O_t = torch.tanh(V_t)\n",
        "        O_t = self.dropout(O_t)\n",
        "        \n",
        "        combined_output = O_t\n",
        "        return dec_state, combined_output, e_t\n",
        "\n",
        "    def generate_sent_masks(self, enc_hiddens: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n",
        "        \"\"\" Generate sentence masks for encoder hidden states.\n",
        "\n",
        "        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n",
        "                                     src_len = max source length, h = hidden size.\n",
        "        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n",
        "\n",
        "        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n",
        "                                    where src_len = max source length, h = hidden size.\n",
        "        \"\"\"\n",
        "        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n",
        "        for e_id, src_len in enumerate(source_lengths):\n",
        "            # fill_mask_ fills positions that are 1\n",
        "            enc_masks[e_id, src_len:] = 1\n",
        "        return enc_masks.to(self.device)\n",
        "\n",
        "    def beam_search(self, src_sent: List[str], beam_size: int=5, \n",
        "                    max_decoding_time_step: int=70) -> List[Hypothesis]:\n",
        "        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n",
        "        @param src_sent (List[str]): a single source sentence (words)\n",
        "        @param beam_size (int): beam size\n",
        "        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n",
        "        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n",
        "                value: List[str]: the decoded target sentence, represented as a list of words\n",
        "                score: float: the log-likelihood of the target sentence\n",
        "        \"\"\"\n",
        "        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n",
        "\n",
        "        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n",
        "        src_encodings_att_linear = self.att_projection(src_encodings)\n",
        "\n",
        "        h_tm1 = dec_init_vec\n",
        "        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n",
        "\n",
        "        eos_id = self.vocab.tgt['</s>']\n",
        "\n",
        "        hypotheses = [['<s>']]\n",
        "        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n",
        "        completed_hypotheses = []\n",
        "\n",
        "        t = 0\n",
        "        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n",
        "            t += 1\n",
        "            hyp_num = len(hypotheses)\n",
        "\n",
        "            exp_src_encodings = src_encodings.expand(hyp_num,\n",
        "                                                     src_encodings.size(1),\n",
        "                                                     src_encodings.size(2))\n",
        "\n",
        "            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n",
        "                                                                           src_encodings_att_linear.size(1),\n",
        "                                                                           src_encodings_att_linear.size(2))\n",
        "\n",
        "            y_tm1 = self.vocab.tgt.to_input_tensor(list([hyp[-1]] for hyp in hypotheses), \n",
        "                                                        device=self.device)\n",
        "            y_t_embed = self.model_embeddings_target(y_tm1)\n",
        "            y_t_embed = torch.squeeze(y_t_embed, dim=0)\n",
        "\n",
        "\n",
        "            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n",
        "\n",
        "            (h_t, cell_t), att_t, _  = self.step(x, h_tm1,\n",
        "                                                exp_src_encodings, exp_src_encodings_att_linear, \n",
        "                                                enc_masks=None)\n",
        "\n",
        "            # log probabilities over target words\n",
        "            log_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n",
        "\n",
        "            live_hyp_num = beam_size - len(completed_hypotheses)\n",
        "            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n",
        "            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n",
        "\n",
        "            prev_hyp_ids = top_cand_hyp_pos / len(self.vocab.tgt)\n",
        "            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n",
        "\n",
        "            new_hypotheses = []\n",
        "            live_hyp_ids = []\n",
        "            new_hyp_scores = []\n",
        "\n",
        "            decoderStatesForUNKsHere = []\n",
        "            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n",
        "                prev_hyp_id = prev_hyp_id.item()\n",
        "                hyp_word_id = hyp_word_id.item()\n",
        "                cand_new_hyp_score = cand_new_hyp_score.item()\n",
        "\n",
        "                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n",
        "\n",
        "                # Record output layer in case UNK was generated\n",
        "                if hyp_word == \"<unk>\":\n",
        "                   hyp_word = \"<unk>\" + str(len(decoderStatesForUNKsHere))\n",
        "                   decoderStatesForUNKsHere.append(att_t[prev_hyp_id])\n",
        "\n",
        "                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n",
        "                if hyp_word == '</s>':\n",
        "                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n",
        "                                                           score=cand_new_hyp_score))\n",
        "                else:\n",
        "                    new_hypotheses.append(new_hyp_sent)\n",
        "                    live_hyp_ids.append(prev_hyp_id)\n",
        "                    new_hyp_scores.append(cand_new_hyp_score)\n",
        "\n",
        "            if len(decoderStatesForUNKsHere) > 0 and self.charDecoder is not None: # decode UNKs\n",
        "                decoderStatesForUNKsHere = torch.stack(decoderStatesForUNKsHere, dim=0)\n",
        "                decodedWords = self.charDecoder.decode_greedy((decoderStatesForUNKsHere.unsqueeze(0), decoderStatesForUNKsHere.unsqueeze(0)), max_length=21, device=self.device)\n",
        "                assert len(decodedWords) == decoderStatesForUNKsHere.size()[0], \"Incorrect number of decoded words\"\n",
        "                for hyp in new_hypotheses:\n",
        "                  if hyp[-1].startswith(\"<unk>\"):\n",
        "                        hyp[-1] = decodedWords[int(hyp[-1][5:])]#[:-1]\n",
        "\n",
        "            if len(completed_hypotheses) == beam_size:\n",
        "                break\n",
        "\n",
        "            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n",
        "            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n",
        "            att_tm1 = att_t[live_hyp_ids]\n",
        "\n",
        "            hypotheses = new_hypotheses\n",
        "            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n",
        "\n",
        "        if len(completed_hypotheses) == 0:\n",
        "            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n",
        "                                                   score=hyp_scores[0].item()))\n",
        "\n",
        "        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n",
        "        return completed_hypotheses\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n",
        "        \"\"\"\n",
        "        return self.att_projection.weight.device\n",
        "\n",
        "    @staticmethod\n",
        "    def load(model_path: str, no_char_decoder=False):\n",
        "        \"\"\" Load the model from a file.\n",
        "        \"\"\"\n",
        "        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "        args = params['args']\n",
        "\n",
        "        model = NMT(vocab = params['vocab'], no_char_decoder=no_char_decoder, **args)\n",
        "        model.load_state_dict(params['state_dict'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def save(self, path: str):\n",
        "        \"\"\" Save the model to a file.\n",
        "        \"\"\"\n",
        "        print('save model parameters to [%s]' % path)\n",
        "\n",
        "        params = {\n",
        "            'args': dict(embed_size = self.model_embeddings_source.embed_size, \n",
        "                         hidden_size = self.hidden_size, \n",
        "                         dropout_rate = self.dropout_rate),\n",
        "            'vocab': self.vocab,\n",
        "            'state_dict': self.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(params, path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeC5AD9oYUHK",
        "colab_type": "text"
      },
      "source": [
        "# char_decoder.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_4CA8yLYWtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Tuple\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK9vZ3g6YfMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharDecoder(nn.Module):\n",
        "    def __init__(self, hidden_size, char_embedding_size = 50, target_vocab = None):\n",
        "        \"\"\" Init Character Decoder.\n",
        "\n",
        "        @param hidden_size (int): Hidden size of the decoder LSTM\n",
        "        @param char_embedding_size (int): dimensionality of character embeddings\n",
        "        @param target_vocab (VocabEntry): vocabulary for the target language. See vocab.py for documentation.\n",
        "        \"\"\"        \n",
        "        super(CharDecoder, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.tgt_vocab = target_vocab\n",
        "        self.vocab_size = len(target_vocab.char2id)\n",
        "\n",
        "        # Unidirectional LSTM\n",
        "        self.charDecoder = nn.LSTM(char_embedding_size, self.hidden_size)\n",
        "        # W_dec and b_dec to compute final value before applying softmax\n",
        "        self.char_output_projection = nn.Linear(self.hidden_size, self.vocab_size, bias = True)\n",
        "        # Char embeddings\n",
        "        tgt_pad_token_idx = target_vocab.char2id['<pad>']\n",
        "        self.decoderCharEmb = nn.Embedding(self.vocab_size, char_embedding_size, \n",
        "                                           padding_idx = tgt_pad_token_idx)\n",
        "\n",
        "    def forward(self, input: torch.tensor, dec_hidden = None):\n",
        "        \"\"\" Forward pass of character decoder.\n",
        "\n",
        "        @param input: tensor of integer indices, shape (seq_length, batch)\n",
        "        @param dec_hidden: internal state of the LSTM before reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)\n",
        "\n",
        "        @returns scores: called s_t in the PDF, shape (seq_length, batch, self.vocab_size)\n",
        "        @returns dec_hidden: internal state of the LSTM after reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)\n",
        "        \"\"\"\n",
        "        chr_embeddings = self.decoderCharEmb(input) # (sequence_length, batch, char_embed_size)\n",
        "        hidden_states, (last_hidden, last_cell) = self.charDecoder(chr_embeddings, dec_hidden) \n",
        "        s_t = self.char_output_projection(hidden_states) # (sequence_length, batch, vocab_size)\n",
        "\n",
        "        dec_hidden = (last_hidden, last_cell)\n",
        "        scores = s_t\n",
        "\n",
        "        return scores, dec_hidden\n",
        "\n",
        "    def train_forward(self, char_sequence, dec_hidden = None):\n",
        "        \"\"\" Forward computation during training.\n",
        "\n",
        "        @param char_sequence: tensor of integers, shape (length, batch). Note that \"length\" here and in forward() need not be the same.\n",
        "        @param dec_hidden: initial internal state of the LSTM, obtained from the output of the word-level decoder. \n",
        "        A tuple of two tensors of shape (1, batch, hidden_size)\n",
        "\n",
        "        @returns The cross-entropy loss, computed as the *sum* of cross-entropy losses of all the words in the batch.\n",
        "        \"\"\"\n",
        "        crossEntropyLoss = nn.CrossEntropyLoss(ignore_index = self.tgt_vocab.char2id['<pad>'], \n",
        "                                               reduction = 'sum')\t# exclude <PAD>\n",
        "\n",
        "        # If I include <END>, the LM will compute the next token given <END> (and previous words)\n",
        "        # however all I need is that the token is an indication of end of sentence\n",
        "        input = char_sequence[:-1]\t# exclude <END>\n",
        "        scores, dec_hidden = self.forward(input, dec_hidden)\n",
        "\n",
        "        input = scores.permute(1, 2, 0)\t# (seq_length, batch, self.vocab_size) to (N, C, d_1, d_2, ..., d_seq_length)\n",
        "        # Because the output of the LM don't have <START> token\n",
        "        target = char_sequence[1:]\t # exclude <START>\n",
        "        target = target.t()\t# (length, batch) to (N, C)\t\n",
        "\n",
        "        loss_char_dec = crossEntropyLoss(input, target)\t# seq_length-dimensional loss\n",
        "\n",
        "        return loss_char_dec\n",
        "\n",
        "    def decode_greedy(self, initial_states: Tuple[torch.tensor], device: torch.device, max_length=21):\n",
        "        \"\"\" Greedy decoding\n",
        "        @param initialStates: initial internal state of the LSTM, a tuple of two tensors of size (1, batch, hidden_size)\n",
        "        @param device: torch.device (indicates whether the model is on CPU or GPU)\n",
        "        @param max_length: maximum length of words to decode\n",
        "\n",
        "        @returns decoded_words: a list (of length batch) of strings, each of which has length <= max_length.\n",
        "                              The decoded strings should NOT contain the start-of-word and end-of-word characters.\n",
        "        \"\"\"\n",
        "        output_words, decoded_words = list(), list()\n",
        "        start_idx = self.tgt_vocab.start_of_word\n",
        "        end_idx = self.tgt_vocab.end_of_word \n",
        "\n",
        "        dec_hiddens = initial_states\n",
        "        batch_size = dec_hiddens[0].shape[1]\n",
        "        current_char = torch.tensor([[start_idx] * batch_size], device = device)\n",
        "\n",
        "        i = 0\n",
        "        while i != max_length:\n",
        "        \tscores, dec_hiddens = self.forward(current_char, dec_hiddens)\n",
        "        \tcurrent_char = scores.argmax(-1)\n",
        "        \toutput_words += [current_char]\n",
        "        \ti += 1\n",
        "        output_words = torch.cat(output_words).t().tolist()\n",
        "\n",
        "        for word in output_words:\n",
        "        \tdecoded_word = ''\n",
        "        \tfor char_idx in word:\n",
        "        \t\tif char_idx == end_idx:\n",
        "        \t\t\tbreak\n",
        "        \t\tdecoded_word += self.tgt_vocab.id2char[char_idx]\n",
        "        \tdecoded_words += [decoded_word]\n",
        "\n",
        "        return decoded_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM0_tEK3LBNS",
        "colab_type": "text"
      },
      "source": [
        "# run.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYLblk0YLdhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import sys\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict, Set, Union\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.utils\n",
        "\n",
        "from collections import namedtuple\n",
        "Hypothesis = namedtuple('Hypothesis', ['value', 'score'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScdLPsSLChG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_ppl(model, dev_data, batch_size=32):\n",
        "    \"\"\" Evaluate perplexity on dev sentences\n",
        "    @param model (NMT): NMT Model\n",
        "    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
        "    @param batch_size (batch size)\n",
        "    @returns ppl (perplixty on dev sentences)\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    cum_loss = 0.\n",
        "    cum_tgt_words = 0.\n",
        "\n",
        "    # no_grad() signals backend to throw away all gradients\n",
        "    with torch.no_grad():\n",
        "        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n",
        "            loss = -model(src_sents, tgt_sents).sum()\n",
        "\n",
        "            cum_loss += loss.item()\n",
        "            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
        "            cum_tgt_words += tgt_word_num_to_predict\n",
        "\n",
        "        ppl = np.exp(cum_loss / cum_tgt_words)\n",
        "\n",
        "    if was_training:\n",
        "        model.train()\n",
        "\n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c7iIjtMLZba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n",
        "    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n",
        "    @param references (List[List[str]]): a list of gold-standard reference target sentences\n",
        "    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n",
        "    @returns bleu_score: corpus-level BLEU score\n",
        "    \"\"\"\n",
        "    if references[0][0] == '<s>':\n",
        "        references = [ref[1:-1] for ref in references]\n",
        "    bleu_score = corpus_bleu([[ref] for ref in references],\n",
        "                             [hyp.value for hyp in hypotheses])\n",
        "                             #smoothing_function = SmoothingFunction().method1)\n",
        "    return bleu_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms7sW-eKOK9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    \"\"\" Train the NMT Model.\n",
        "    \"\"\"\n",
        "    train_data_src = read_corpus(args['--train-src'], source='src')\n",
        "    train_data_tgt = read_corpus(args['--train-tgt'], source='tgt')\n",
        "\n",
        "    dev_data_src = read_corpus(args['--dev-src'], source='src')\n",
        "    dev_data_tgt = read_corpus(args['--dev-tgt'], source='tgt')\n",
        "\n",
        "    train_data = list(zip(train_data_src, train_data_tgt))\n",
        "    dev_data = list(zip(dev_data_src, dev_data_tgt))\n",
        "\n",
        "    train_batch_size = args['--batch-size']\n",
        "\n",
        "    clip_grad = args['--clip-grad']\n",
        "    valid_niter = args['--valid-niter']\n",
        "    log_every = args['--log-every']\n",
        "    model_save_path = args['--save-to']\n",
        "\n",
        "    vocab = Vocab.load(args['--vocab'])\n",
        "\n",
        "    model = NMT(embed_size=int(args['--embed-size']),\n",
        "                hidden_size=int(args['--hidden-size']),\n",
        "                dropout_rate=float(args['--dropout']),\n",
        "                vocab=vocab, no_char_decoder=args['--no-char-decoder'])\n",
        "    model.train()\n",
        "\n",
        "    uniform_init = args['--uniform-init']\n",
        "    if np.abs(uniform_init) > 0.:\n",
        "        print('uniformly initialize parameters [-%f, +%f]' % (uniform_init, uniform_init))\n",
        "        for p in model.parameters():\n",
        "            p.data.uniform_(-uniform_init, uniform_init)\n",
        "\n",
        "    vocab_mask = torch.ones(len(vocab.tgt))\n",
        "    vocab_mask[vocab.tgt['<pad>']] = 0\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if args['--cuda'] else \"cpu\")\n",
        "    print('use device: %s' % device)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = args['--lr'])\n",
        "\n",
        "    num_trial = 0\n",
        "    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n",
        "    cum_examples = report_examples = epoch = valid_num = 0\n",
        "    hist_valid_scores = []\n",
        "    train_time = begin_time = time.time()\n",
        "    print('Begin Maximum Likelihood training')\n",
        "    print(80*'-+')\n",
        "\n",
        "    while True:\n",
        "        epoch += 1\n",
        "        # Take out two sentences for each epoch\n",
        "        for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n",
        "            #print('src_sents: ', src_sents)\n",
        "            #print('tgt_sents: ', tgt_sents)\n",
        "            train_iter += 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_size = len(src_sents)\n",
        "\n",
        "            example_losses = -model(src_sents, tgt_sents) # (batch_size,)\n",
        "            batch_loss = example_losses.sum()\n",
        "            loss = batch_loss / batch_size\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # clip gradient\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_losses_val = batch_loss.item()\n",
        "            report_loss += batch_losses_val\n",
        "            cum_loss += batch_losses_val\n",
        "\n",
        "            tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
        "            report_tgt_words += tgt_words_num_to_predict\n",
        "            cum_tgt_words += tgt_words_num_to_predict\n",
        "            report_examples += batch_size\n",
        "            cum_examples += batch_size\n",
        "\n",
        "            if train_iter % log_every == 0:\n",
        "                print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n",
        "                      'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,\n",
        "                                                                                         report_loss / report_examples,\n",
        "                                                                                         math.exp(report_loss / report_tgt_words),\n",
        "                                                                                         cum_examples,\n",
        "                                                                                         report_tgt_words / (time.time() - train_time),\n",
        "                                                                                         time.time() - begin_time), file=sys.stderr)\n",
        "\n",
        "                train_time = time.time()\n",
        "                report_loss = report_tgt_words = report_examples = 0.\n",
        "\n",
        "            # perform validation\n",
        "            if train_iter % valid_niter == 0:\n",
        "                print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,\n",
        "                                                                                         cum_loss / cum_examples,\n",
        "                                                                                         np.exp(cum_loss / cum_tgt_words),\n",
        "                                                                                         cum_examples), file=sys.stderr)\n",
        "\n",
        "                cum_loss = cum_examples = cum_tgt_words = 0.\n",
        "                valid_num += 1\n",
        "\n",
        "                print('begin validation ...')\n",
        "\n",
        "                # compute dev. ppl and bleu\n",
        "                dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n",
        "                valid_metric = -dev_ppl\n",
        "\n",
        "                print('validation: iter %d, dev. ppl %f' % (train_iter, dev_ppl))\n",
        "\n",
        "                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n",
        "                hist_valid_scores.append(valid_metric)\n",
        "\n",
        "                if is_better:\n",
        "                    patience = 0\n",
        "                    print('save currently the best model to [%s]' % model_save_path)\n",
        "                    model.save(model_save_path)\n",
        "\n",
        "                    # also save the optimizers' state\n",
        "                    torch.save(optimizer.state_dict(), model_save_path + '.optim')\n",
        "                elif patience < args['--patience']:\n",
        "                    patience += 1\n",
        "                    print('hit patience %d' % patience)\n",
        "\n",
        "                    if patience == args['--patience']:\n",
        "                        num_trial += 1\n",
        "                        print('hit #%d trials' % num_trial)\n",
        "                        if num_trial == args['--max-num-trial']:\n",
        "                            print('early stop!')\n",
        "                            exit(0)\n",
        "\n",
        "                        # decay lr, and restore from previously best checkpoint\n",
        "                        lr = optimizer.param_groups[0]['lr'] * args['--lr-decay']\n",
        "                        print('load previously best model and decay learning rate to %f' % lr)\n",
        "\n",
        "                        # load model\n",
        "                        params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n",
        "                        model.load_state_dict(params['state_dict'])\n",
        "                        model = model.to(device)\n",
        "\n",
        "                        print('restore parameters of the optimizers')\n",
        "                        optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n",
        "\n",
        "                        # set new lr\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr\n",
        "\n",
        "                        # reset patience\n",
        "                        patience = 0\n",
        "\n",
        "            if epoch == args['--max-epoch']:\n",
        "                print('reached maximum number of epochs!', file=sys.stderr)\n",
        "                return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIgaxrVMLbcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode(args: Dict[str, str]):\n",
        "    \"\"\" Performs decoding on a test set, and save the best-scoring decoding results.\n",
        "    If the target gold-standard sentences are given, the function also computes\n",
        "    corpus-level BLEU score.\n",
        "    @param args (Dict): args from cmd line\n",
        "    \"\"\"\n",
        "    # Load test data, source sentences and/or target sentences\n",
        "    print(\"load test source sentences from [{}]\".format(args['TEST_SOURCE_FILE']), file=sys.stderr)\n",
        "    test_data_src = read_corpus(args['TEST_SOURCE_FILE'], source='src')\n",
        "    if args['TEST_TARGET_FILE']:\n",
        "        print(\"load test target sentences from [{}]\".format(args['TEST_TARGET_FILE']), file=sys.stderr)\n",
        "        test_data_tgt = read_corpus(args['TEST_TARGET_FILE'], source='tgt')\n",
        "\n",
        "    # Load model\n",
        "    print(\"load model from {}\".format(args['MODEL_PATH']), file=sys.stderr)\n",
        "    model = NMT.load(args['MODEL_PATH'], no_char_decoder=args['--no-char-decoder'])\n",
        "\n",
        "    # Check if user wants to user GPU\n",
        "    if args['--cuda']:\n",
        "        model = model.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "    # Apply hypotheses finding algorithm, here I use beam search\n",
        "    hypotheses = beam_search(model, test_data_src,\n",
        "                             beam_size=int(args['--beam-size']),\n",
        "                             max_decoding_time_step=int(args['--max-decoding-time-step']))\n",
        "    print('hypotheses: ', hypotheses)\n",
        "\n",
        "    # If I loaded test target sentences, compute BLEU scores between translated sentences and correct ones\n",
        "    if args['TEST_TARGET_FILE']:\n",
        "        top_hypotheses = [hyps[0] for hyps in hypotheses]\n",
        "        bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n",
        "        print('Corpus BLEU: {}'.format(bleu_score * 100), file=sys.stderr)\n",
        "\n",
        "    # Write translated sentences to file\n",
        "    with open(args['OUTPUT_FILE'], 'w') as f:\n",
        "        for src_sent, hyps in zip(test_data_src, hypotheses):\n",
        "            top_hyp = hyps[0]\n",
        "            hyp_sent = ' '.join(top_hyp.value)\n",
        "            f.write(hyp_sent + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teNBlM3oMQVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n",
        "    \"\"\" Run beam search to construct hypotheses for a list of src-language sentences.\n",
        "    @param model (NMT): NMT Model\n",
        "    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n",
        "    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n",
        "    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n",
        "    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    hypotheses = []\n",
        "    with torch.no_grad():\n",
        "        for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):\n",
        "            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n",
        "\n",
        "            hypotheses.append(example_hyps)\n",
        "\n",
        "    if was_training: model.train(was_training)\n",
        "\n",
        "    return hypotheses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhvkhOz8Z5Jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab.load(root + 'Deep learning in NLP/Nom Translator/corpora/nom_vlatin_vocab.json')\n",
        "a, b = vocab.get_word2id()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LkHbIXgaQwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv9JgMjGaRaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjY-89YOk3F_",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAx5tRoYMoXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "     # seed the random number generators\n",
        "    seed = 103\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    cuda = True\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed * 13 // 7)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iQa_t8UvZpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "    '--train-src': root + 'Deep learning in NLP/Nom Translator/corpora/1. train.nom',\n",
        "    '--train-tgt': root + 'Deep learning in NLP/Nom Translator/corpora/2. train.vlatin',\n",
        "    '--dev-src': root + 'Deep learning in NLP/Nom Translator/corpora/3. dev.nom',\n",
        "    '--dev-tgt': root + 'Deep learning in NLP/Nom Translator/corpora/4. dev.vlatin',\n",
        "    '--batch-size': 32,\n",
        "    '--clip-grad': 5.0,\n",
        "    '--valid-niter': 100,\n",
        "    '--log-every': 10,\n",
        "    '--save-to': root + 'Deep learning in NLP/Nom Translator/models/nom_vlatin_model_13.bin',\n",
        "    '--vocab': root + 'Deep learning in NLP/Nom Translator/corpora/nom_vlatin_vocab.json',\n",
        "    '--embed-size': 1024,\n",
        "    '--hidden-size': 512,\n",
        "    '--dropout': .3,\n",
        "    '--uniform-init': .1,\n",
        "    '--cuda': True,\n",
        "    '--lr': .001 ,\n",
        "    '--max-epoch': 10,\n",
        "    '--patience': 5,\n",
        "    '--lr-decay': .5,\n",
        "    '--max-num-trial': 5,\n",
        "    '--no-char-decoder': False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx_k6OUrYxtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hutZyICsyRiY",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-mJXC8qSWoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {'TEST_SOURCE_FILE': root + 'Deep learning in NLP/Nom Translator/corpora/5. test.nom',\n",
        "        'TEST_TARGET_FILE': root + 'Deep learning in NLP/Nom Translator/corpora/6. test.vlatin',\n",
        "        'MODEL_PATH': root + 'Deep learning in NLP/Nom Translator/models/nom_vlatin_model_13.bin',\n",
        "        '--no-char-decoder': False,\n",
        "        '--cuda': True,\n",
        "        '--beam-size': 5,\n",
        "        '--max-decoding-time-step': 70,\n",
        "        'OUTPUT_FILE': root + 'Deep learning in NLP/Nom Translator/results/test_outputs_10.txt'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3kfweFhTlhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decode(args)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}